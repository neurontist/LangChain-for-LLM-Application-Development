{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ8QBlxQnQ+FDef9asDrHw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neurontist/LangChain-for-LLM-Application-Development/blob/main/L2%20Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain: Memory\n",
        "**Outline**\n",
        "\n",
        "* ConversationBufferMemory\n",
        "* ConversationBufferWindowMemory\n",
        "* ConversationTokenBufferMemory\n",
        "* ConversationSummaryMemory\n"
      ],
      "metadata": {
        "id": "h6BxiwE6eVl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-genai\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "h4jMcRoqssDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e70d4a8-8016-4a37-a5c4-1ff36c0ca7b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1Xu6N025qmoB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "EExr0-4OvK_Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationBufferMemory\n",
        "\n",
        "This memory allows for storing of messages and then extracts the messages in a variable"
      ],
      "metadata": {
        "id": "CS8CarQRvUgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "8MfvJKV1e3MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "CYgZ7eVjrAFq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Model and Memory"
      ],
      "metadata": {
        "id": "QV5sDTboe5v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = \"gemini-1.5-flash\""
      ],
      "metadata": {
        "id": "PaeH4cQ-rRG9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(temperature=0, model=llm_model, api_key=GOOGLE_API_KEY)\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm = llm,\n",
        "    memory = memory,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "6A0cLd7xrYS_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding How This type of Memory Works\n",
        "\n",
        "Verbose to true value helps to understand the LLM processing logic"
      ],
      "metadata": {
        "id": "pwBwsUY1fCAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi, my name is Lora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "collapsed": true,
        "id": "kyVSkdd7rvSj",
        "outputId": "8a436606-5f16-4131-ae75-0394f8858082"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Lora\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi Lora! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, albeit somewhat literal, conversational partner.  So, what can I help you with today?  Are you looking for information on a specific topic, or just wanting to chat?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "collapsed": true,
        "id": "AXKfwbTYrxF2",
        "outputId": "6b7dd75e-3795-4d4a-a458-37e140d58aba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Lora\n",
            "AI: Hi Lora! It's nice to meet you.  My name isn't really a \"name\" in the human sense, as I don't have a personal identity like you do.  I'm a large language model, trained by Google.  I don't have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, albeit somewhat literal, conversational partner.  So, what can I help you with today?  Are you looking for information on a specific topic, or just wanting to chat?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"1 + 1 = 2.  That's a pretty straightforward calculation!  My training data includes a vast amount of mathematical information, so simple arithmetic problems like this are easily within my capabilities.  Is there anything else I can help you with? Perhaps a more complex equation, or a completely different topic altogether? I'm happy to discuss pretty much anything, within the boundaries of my programming, of course.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "collapsed": true,
        "id": "ropGEtpFrxCi",
        "outputId": "5d8b0358-070d-44c2-e345-b8d1a30417f9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Lora\n",
            "AI: Hi Lora! It's nice to meet you.  My name isn't really a \"name\" in the human sense, as I don't have a personal identity like you do.  I'm a large language model, trained by Google.  I don't have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, albeit somewhat literal, conversational partner.  So, what can I help you with today?  Are you looking for information on a specific topic, or just wanting to chat?\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 = 2.  That's a pretty straightforward calculation!  My training data includes a vast amount of mathematical information, so simple arithmetic problems like this are easily within my capabilities.  Is there anything else I can help you with? Perhaps a more complex equation, or a completely different topic altogether? I'm happy to discuss pretty much anything, within the boundaries of my programming, of course.\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Your name is Lora.  You told me that at the beginning of our conversation.  I have a good memory for details within our current interaction, although I don't retain information from previous conversations.  Is there anything else I can assist you with?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memory of Chat retained"
      ],
      "metadata": {
        "id": "4XxSJtK3ftyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LbwSy-Orrw_1",
        "outputId": "61d42b74-8850-410d-c02e-241c7a0e1019"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Lora\n",
            "AI: Hi Lora! It's nice to meet you.  My name isn't really a \"name\" in the human sense, as I don't have a personal identity like you do.  I'm a large language model, trained by Google.  I don't have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, albeit somewhat literal, conversational partner.  So, what can I help you with today?  Are you looking for information on a specific topic, or just wanting to chat?\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 = 2.  That's a pretty straightforward calculation!  My training data includes a vast amount of mathematical information, so simple arithmetic problems like this are easily within my capabilities.  Is there anything else I can help you with? Perhaps a more complex equation, or a completely different topic altogether? I'm happy to discuss pretty much anything, within the boundaries of my programming, of course.\n",
            "Human: What is my name?\n",
            "AI: Your name is Lora.  You told me that at the beginning of our conversation.  I have a good memory for details within our current interaction, although I don't retain information from previous conversations.  Is there anything else I can assist you with?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sPzy3msfrw9D",
        "outputId": "3adad66d-bb5c-4459-ca70-6e74e0861666"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Hi, my name is Lora\\nAI: Hi Lora! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, albeit somewhat literal, conversational partner.  So, what can I help you with today?  Are you looking for information on a specific topic, or just wanting to chat?\\nHuman: What is 1+1?\\nAI: 1 + 1 = 2.  That\\'s a pretty straightforward calculation!  My training data includes a vast amount of mathematical information, so simple arithmetic problems like this are easily within my capabilities.  Is there anything else I can help you with? Perhaps a more complex equation, or a completely different topic altogether? I\\'m happy to discuss pretty much anything, within the boundaries of my programming, of course.\\nHuman: What is my name?\\nAI: Your name is Lora.  You told me that at the beginning of our conversation.  I have a good memory for details within our current interaction, although I don\\'t retain information from previous conversations.  Is there anything else I can assist you with?'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory()"
      ],
      "metadata": {
        "id": "lFvNyNkhrw6S"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ],
      "metadata": {
        "id": "g_qvltuhrwyp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eciVN-ZrrwvQ",
        "outputId": "33d9af81-7d37-4a60-be20-f61a64d32798"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3eI4f8Srwsw",
        "outputId": "4d1db540-b2ee-4087-b067-8d8c15d89298"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\"}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ],
      "metadata": {
        "id": "5zzJiH3mrwqC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uROPDbjfrwnV",
        "outputId": "e85b5834-99f1-41ef-c052-a787df18de39"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationBufferWindowMemory\n",
        "\n",
        "This memory keeps a list of the interactions of the conversation over time. It only uses the last K interactions."
      ],
      "metadata": {
        "id": "yQL6bMF-vf01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "nK_IJti8rwfR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(temperature=0, model=llm_model,api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "OgmnWcO6yWXi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)"
      ],
      "metadata": {
        "id": "5E6IKQnjrwMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7461a59a-5762-493f-fa1b-7e075d73d196"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-edd6ddba5fe5>:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm = llm,\n",
        "    memory = memory,\n",
        "    verbose = False\n",
        ")"
      ],
      "metadata": {
        "id": "AoZQKa8UwJNd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi, my name is Aliza\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "0BOkDevsxN7c",
        "outputId": "19ef4c2c-4bd3-4ce3-983c-7c71a583147b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi Aliza! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  So, while I can\\'t tell you about my weekend or what my favorite color is, I can tell you about the history of the color blue, or the latest research on artificial intelligence, or even help you write a poem!  What would you like to talk about today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "collapsed": true,
        "outputId": "f79f973a-1be3-41bb-c43b-578307aec321",
        "id": "HhQJS8d3xF4h"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"1 + 1 = 2.  That's a fundamental arithmetic operation.  It's based on the Peano axioms, which define the natural numbers and their successor function.  In essence, the successor of 1 is 2.  This is a very basic concept, taught in early elementary school mathematics around the world.  It's the foundation for much more complex mathematical concepts and calculations.  Is there anything else I can help you with?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Za8aRdt2xN4J",
        "outputId": "6e9b777c-a4a3-4ca9-c70a-ed7d13e7ee7c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I do not know your name.  I have no access to personal information about you unless you have explicitly provided it during our current conversation.  My knowledge is based on the vast dataset I was trained on, but that dataset does not include private details about individuals.  To help me answer your questions more effectively in the future, please feel free to share information with me, but remember that I will not retain that information after our conversation ends unless you explicitly instruct me to do so (and even then, my ability to do so is limited by my design and security protocols).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationTokenBufferMemory\n",
        "\n",
        "This memory keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions."
      ],
      "metadata": {
        "id": "zaRohkJByCoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory"
      ],
      "metadata": {
        "id": "dPxtxvtExN2Z"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)"
      ],
      "metadata": {
        "id": "bCm1lKtIxN0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ed0ccc-5dbe-4cf4-f2de-88d120cb3d06"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-a5f6e96642c3>:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})"
      ],
      "metadata": {
        "id": "JQpR7PXRxNyA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYsgp8PGxNvo",
        "outputId": "4ca9798e-3fe8-4c65-b199-6d4a32d0c089"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationSummaryBufferMemory\n",
        "\n",
        "This memory creates a summary of the conversation over time"
      ],
      "metadata": {
        "id": "3Qub0At1y9Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ],
      "metadata": {
        "id": "dQihSsjzxNtF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a long string\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n"
      ],
      "metadata": {
        "id": "OqoFTxjGxNqn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)"
      ],
      "metadata": {
        "id": "SRUDiTPQxNn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aac6e0f-44a2-4911-a70c-d3e49dcc4fbc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-84069f0aca94>:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ],
      "metadata": {
        "id": "bDj1gPuSxNbF"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqUZIGOVzUyW",
        "outputId": "6488dd66-12e2-49c4-d82b-edec754ac55b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'System: The human greets the AI, and after some casual exchange, asks what is on the schedule for the day.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "xEVCNjhVzXzf"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "Q5JBxTIqz-5W"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = \"\"\"\n",
        "Generate a demo to show to the customer\\\n",
        "the output should be in numbered points and must in the following format\\\n",
        "1. Idea\n",
        "2. Application\n",
        "3. Conclusion\n",
        "{text}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BRMkdc3wz-2A"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(demo)"
      ],
      "metadata": {
        "id": "sFsqCSk0z-yU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_suggestion = \"What would be a good demo to show for a langchain AI Agent in concise way?\""
      ],
      "metadata": {
        "id": "UACx5R2V04PV"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = prompt_template.format_messages(text=demo_suggestion)"
      ],
      "metadata": {
        "id": "NGuUR994z-v2"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(message)"
      ],
      "metadata": {
        "id": "TVpUFOvFz-tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fccea72-1306-4166-97da-adcc8d6ab906"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-54fecca17caa>:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm(message)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1VWVAH3nz-qu",
        "outputId": "06f4ef0d-7347-4590-c7a4-a51974f2f2bf"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a demo showcasing a LangChain AI agent, broken down into the numbered points you requested:\n",
            "\n",
            "**1. Idea:**  Building a personalized travel itinerary based on user preferences.\n",
            "\n",
            "**Application:** (This section would involve a live demonstration, but here's a script outlining what would happen)\n",
            "\n",
            "* **The Agent's Capabilities:**  We'll use a LangChain agent that combines several tools:\n",
            "    * **A Large Language Model (LLM):**  For natural language understanding and itinerary generation.\n",
            "    * **A Web Search Tool:** To gather information about locations, attractions, and transportation.\n",
            "    * **A Calendar API (optional, for more advanced demos):** To check for availability of flights and hotels.\n",
            "\n",
            "* **The User Interaction:**  I'll ask the agent to create a 5-day trip to Italy for a couple interested in history, art, and good food, with a budget of $3000.  (The specific prompt would be shown on screen).\n",
            "\n",
            "* **The Agent's Response:** The agent will then:\n",
            "    * Use the LLM to understand the user's request.\n",
            "    * Use the web search tool to find relevant information about Italy (cities, attractions, restaurants).\n",
            "    * Potentially use the calendar API to check flight and hotel availability (if implemented).\n",
            "    * Generate a detailed itinerary including suggested locations, activities, restaurants, and estimated costs.  (This itinerary would be displayed on screen).  The itinerary might look something like this (simplified example):\n",
            "\n",
            "    * **Day 1:** Arrive in Rome, check into hotel near the Colosseum.  Evening: Dinner at Trattoria Monti.\n",
            "    * **Day 2:** Visit the Colosseum and Roman Forum. Afternoon: Explore the Vatican City. Evening:  Pasta making class.\n",
            "    * **Day 3:** High-speed train to Florence. Check into hotel near the Duomo. Evening:  Dinner with a view of the Ponte Vecchio.\n",
            "    * **Day 4:** Visit the Uffizi Gallery and climb the Duomo. Afternoon: Explore the Ponte Vecchio. Evening:  Gelato tasting tour.\n",
            "    * **Day 5:** Departure from Florence.\n",
            "\n",
            "\n",
            "**3. Conclusion:**\n",
            "\n",
            "* This demo showcases how LangChain can combine different tools to create a powerful and versatile AI agent capable of performing complex tasks.  The agent's ability to understand natural language, access external information, and generate creative outputs makes it ideal for applications like personalized travel planning, customer service, research assistance, and more.\n",
            "* The modular nature of LangChain allows for easy customization and extension with additional tools and LLMs, making it a highly adaptable solution for various needs.  We can tailor this to your specific requirements by integrating other APIs or data sources.\n",
            "\n",
            "\n",
            "**Important Note for the Demo:**  Prepare the demo beforehand.  Ensure all the necessary tools (LLM, web search, potentially calendar API) are properly configured and working.  Have a pre-written prompt ready to go, and anticipate potential issues (e.g., API rate limits, slow internet connection).  A smooth, well-rehearsed demo is crucial for a positive customer impression.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Used Output Parser for practice purpose"
      ],
      "metadata": {
        "id": "2Xy5w2LF4rA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
      ],
      "metadata": {
        "id": "b04fA_9gz-dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idea_schema = ResponseSchema(name=\"1.\",\n",
        "                             description=\"It should put forth the main idea to build the AI agent\")\n",
        "application_schema = ResponseSchema(name=\"2.\",\n",
        "                             description=\"It should suggest why to build it for this use case\")\n",
        "conclusion_schema = ResponseSchema(name=\"3.\",\n",
        "                             description=\"It should conclude with one line\")"
      ],
      "metadata": {
        "id": "tb_O3J3Q10g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_schemas = [idea_schema, application_schema, conclusion_schema]"
      ],
      "metadata": {
        "id": "iy-fGwBE10du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructuredOutputParser.from_response_schemas(response_schemas=response_schemas)"
      ],
      "metadata": {
        "id": "Npwervp310ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_instructions = schema.get_format_instructions()"
      ],
      "metadata": {
        "id": "UEL5qo2f10ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = \"\"\"\n",
        "Generate a demo to show to the customer\\\n",
        "the output should be in numbered points and must in the following format\\\n",
        "1. Idea\n",
        "2. Application\n",
        "3. Conclusion\n",
        "{text}\n",
        "{format_instructions}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "U3vhN8Of10Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(demo)"
      ],
      "metadata": {
        "id": "-qUkpQOz10UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_suggestion = \"What would be a good demo to show for a langchain AI Agent in concise way?\""
      ],
      "metadata": {
        "id": "qPAUW6Io10R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = prompt_template.format_messages(text=demo_suggestion, format_instructions=format_instructions)"
      ],
      "metadata": {
        "id": "ssqLWJaU10PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(message)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_CFPKSOzoME",
        "outputId": "bb17e9da-05e1-4640-9398-39b9baeedbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\t\"1.\": \"Build a LangChain AI agent that can answer questions about your company's internal documentation (e.g., onboarding materials, product specs, engineering design documents).  The agent will ingest the documents, create an index, and then use a Large Language Model (LLM) to query the index and provide relevant, concise answers.\",\n",
            "\t\"2.\": \"This agent addresses the common problem of employees struggling to find information quickly within large amounts of internal documentation.  It provides a natural language interface for querying, making information access faster and more efficient, boosting productivity and reducing onboarding time.\",\n",
            "\t\"3.\": \"A LangChain AI agent empowers employees with quick and easy access to critical internal knowledge.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict = schema.parse(response.content)"
      ],
      "metadata": {
        "id": "Oiigq0nj3t0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict.get(\"3.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_pI5KAzJ3x_M",
        "outputId": "6805eac1-6c2a-4c74-efa0-5e9211ef63d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A LangChain AI agent empowers employees with quick and easy access to critical internal knowledge.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Memory Types\n",
        "\n",
        "Vector data memory\n",
        "* stores text (from conversation or elsewhere) in  a vector database and retrieves the most relevant blocks of text.\n",
        "\n",
        "Entity memories\n",
        "* Using a LLM, it remembers details about specific entities.\n",
        "\n",
        "You can also use multiple memories at one time.\n",
        "E.g. Conversation memory + Entity memory to recall individuals.\n",
        "\n",
        "You can also store the conversation in a conventional database (such as key-value store or SQL)"
      ],
      "metadata": {
        "id": "iARaud2ehWQg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_oqOUcN4Ye9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
